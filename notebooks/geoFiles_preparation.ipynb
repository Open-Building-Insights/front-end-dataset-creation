{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geoFiles_preparation\n",
    "### This notebook generates the geoJSON, ZIP archives and geoparquet files related to counties/districts used in the OBI tool in the \"Select an area\" option\n",
    "### last_processed_idx can be used to resume a failed computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "#### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n",
    "    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    \"COS_APIKEY\": \"xxx\",\n",
    "    \"PRECREATED_GEOJSON_BUCKET\": \"counties-geojsons\",\n",
    "    \"DB2_CONNECTION_STRING\": \"jdbc:db2://65beb513-5d3d-4101-9001-f42e9dc954b3.brt9d04f0cmqeb8u7740.databases.appdomain.cloud:30371/BLUDB:sslConnection=true;useJDBC4ColumnNameAndLabelSemantics=false;db2.jcc.charsetDecoderEncoder=3;\",\n",
    "    \"DB2_USERNAME\": \"xxx\",\n",
    "    \"DB2_PASSWORD\": \"xxx\",\n",
    "    \"COUNTRY_TABLE\": \"FEATURES_DB_VIDA_EXTENDED\",\n",
    "    \"SHARE_PARQUET_BUCKET\": \"districts-geoparquets\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read notebook configuration\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a README.txt file so it can be added to the zip file later\n",
    "readme_txt_text = '''\n",
    "Attribute Dictionary:\n",
    "- latitude --> latitude of the centroid of the building in degrees\n",
    "- longitude --> longitude of the centroid of the building in degrees\n",
    "- source --> Source of the building footprint (Google Open Buildings | Microsoft Building Footprints | OSM)\n",
    "- urban_split --> Urban classification (Urban | Sub-urban | Rural)\n",
    "- ghsl_smod --> Degree of Urbanisation derived by GHS-SMOD R2023A - GHS settlement layers\n",
    "- res_type --> Residential Vs non-Residential building as per classification source\n",
    "- type_source --> Source of the classification output (classification model | OSM derived)\n",
    "- osm_type --> Tag if building classification is OpenStreetMap derived\n",
    "- confidence --> Confidence value of the classification model (in percentage)\n",
    "- height --> Height of the building in meters derived from WSF3DV3\n",
    "- floors --> Estimated number of floors based on height of the building assuming ~3m/floor (first floor assumed ~4.5m)\n",
    "- building_faces --> Estimated number of outer walls (faces) of the building (based on its geometry)\n",
    "- perimeter_in_meters --> Estimated total length of the outer walls of the building in meters (based on its geometry)\n",
    "- area_in_meters --> Estimated roof area of the building in square meters (when viewed from top based on its geometry)\n",
    "- gfa_in_meters --> Gross floor area in square meters: Estimated based on rooftop area times the amount of floors\n",
    "- elec_access_percent --> Estimated mean likelihood that the building of interest is connected to the electric grid, derived from Open Energy Maps\n",
    "- elec_consumption_kWh_month --> Mean-point estimate of a modelled distribution curve of monthly electricity consumption for a building, given in kWh, derived from Open Energy Maps\n",
    "- elec_consumption_std_kWh_month --> Standard deviation of the monthly electricity consumption value, derived from Open Energy Maps\n",
    "\n",
    "\n",
    "Sources & References:\n",
    "- Sentinel-2 Cloud-Optimized GeoTIFFs: Sentinel-2 satellite images are downloaded from the public S3 bucket Sentinel-2 Cloud-Optimized GeoTIFFs containing satellite images of the Earth’s surface divided into pre-defined tiles (https://sentiwiki.copernicus.eu/web/s2-mission).\n",
    "\n",
    "- German Aerospace Center (DLR): Provides WSF3DV3 data layer that is used for the building height calculation process (https://geoservice.dlr.de/web/maps/eoc:wsf3d) & Esch, Brzoska, Dech, Leutner, Palacios-Lopez, Metz-Marconcini, Marconcini, Roth and Zeidler, 2022: \"World Settlement Footprint 3D - A first three-dimensional survey of the global building stock\".\n",
    "\n",
    "- Google-Microsoft Open Buildings (combined and published by VIDA): Publicly available data contain a catalogue of buildings with specific coordinates and polygons (i.e. shapes of the buildings) for any given country or region (https://source.coop/repositories/vida/google-microsoft-open-buildings/description).\n",
    "\n",
    "- GHS Settlement Model Grid: Publicly available data downloaded as GeoTIF to categorize buildings into Urban or Rural categories (https://human-settlement.emergency.copernicus.eu/download.php?ds=smod) || Schiavina, Marcello; Melchiorri, Michele; Pesaresi, Martino (2023): GHS-SMOD R2023A - GHS settlement layers, application of the Degree of Urbanisation methodology (stage I) to GHS-POP R2023A and GHS-BUILT-S R2023A, multitemporal (1975-2030). European Commission, Joint Research Centre (JRC) [Dataset] doi: 10.2905/A0DF7A6F-49DE-46EA-9BDE-563437A6E2BA PID: http://data.europa.eu/89h/a0df7a6f-49de-46ea-9bde-563437a6e2ba || Concept & Methodology: European Commission, and Statistical Office of the European Union, 2021. Applying the Degree of Urbanisation — A methodological manual to define cities, towns and rural areas for international comparisons — 2021 edition Publications Office of the European Union, 2021, ISBN 978-92-76-20306-3 doi: 10.2785/706535/\n",
    "\n",
    "- Open Street Map (OSM): Publicly available data contain a catalogue of buildings with specific coordinates and polygons (i.e. shapes of the buildings). Data are downloaded as shapefiles (.shp) from geofabrik.de.\n",
    "\n",
    "- Ookla’s Open Data: Open data sets available on a complimentary basis to help people make informed decisions around internet connectivity and internet speed (https://www.ookla.com/ookla-for-good/open-data).\n",
    "\n",
    "- Overture Maps: Publicly available data contain a catalogue of buildings with specific coordinates and polygons (i.e. shapes of the buildings) (https://overturemaps.org/).\n",
    "\n",
    "- Open Energy Maps: Providing building-level electricity access and consumption estimates for Kenya (https://www.openenergymaps.org/) & Lee, S.J., 2023: \"Multimodal Data Fusion for Estimating Electricity Access and Demand\" (Doctoral dissertation, Massachusetts Institute of Technology).\n",
    "\n",
    "\n",
    "Note: The data is shared under the Open Data Commons Open Database License (ODbL) v1.0 license (https://opendatacommons.org/licenses/odbl/1-0/). \n",
    "\n",
    "\n",
    "This work was supported and built by SEforALL, DLR, Open Energy Maps, and Mahila Housing Trust through IBM Sustainability Accelerator program (2024). More info about the programme is available here https://www.ibm.com/impact/initiatives/ibm-sustainability-accelerator.\n",
    "\n",
    "'''\n",
    "\n",
    "with open('README.txt', 'w') as f:\n",
    "    f.write(readme_txt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import geopandas as gpd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapely\n",
    "from collections import Counter\n",
    "import jaydebeapi as jdbc\n",
    "import jpype\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from botocore.client import Config\n",
    "from pyproj import Geod\n",
    "import ibm_boto3\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init S3 client in order to work with last tiff file version\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "                            #   ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n",
    "geod = Geod(ellps=\"WGS84\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the IBM DB2 function\n",
    "def connect_to_db():\n",
    "\n",
    "    jar = 'db2jcc4.jar'\n",
    "    os.environ['CLASSPATH'] = jar\n",
    "\n",
    "    args='-Djava.class.path=%s' % jar\n",
    "    jvm_path = jpype.getDefaultJVMPath()\n",
    "    try:\n",
    "        jpype.startJVM(jvm_path, args)\n",
    "    except Exception as e:\n",
    "        print('startJVM exception: ', e)\n",
    "        \n",
    "    if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n",
    "        jpype.attachThreadToJVM()\n",
    "        jpype.java.lang.Thread.currentThread().setContextClassLoader(jpype.java.lang.ClassLoader.getSystemClassLoader())\n",
    "        \n",
    "    # create JDBC connection\n",
    "    conn = jdbc.connect(\n",
    "                'com.ibm.db2.jcc.DB2Driver',\n",
    "                config['DB2_CONNECTION_STRING'],\n",
    "                [config[\"DB2_USERNAME\"], config[\"DB2_PASSWORD\"]],\n",
    "                'db2jcc4.jar')\n",
    "    \n",
    "    return conn\n",
    "\n",
    "conn = connect_to_db()\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath that defines the source of districts polygons\n",
    "# in district_boundaries folder here are shapefiles and geojsons that defines administrative bouders for selected region\n",
    "# Maharashtra districts are represented by geojson from source https://github.com/HindustanTimesLabs/shapefiles/blob/master/state_ut/maharashtra/district/maharashtra_district.json\n",
    "# Kenya counties are repesented by shapefile\n",
    "filepath = 'boundaries/ke_county.shp'\n",
    "\n",
    "if '.shp'in filepath:\n",
    "    shapefile = gpd.read_file(filepath)\n",
    "    # rename  county column in dataframe\n",
    "    shapefile = shapefile.rename(columns={'county': 'district'})\n",
    "\n",
    "elif '.json'in filepath and 'Maharashtra' in filepath:\n",
    "    shapefile = gpd.read_file(open(filepath))\n",
    "    # add country column\n",
    "    shapefile['country'] = ['Maharashtra' for _ in  range(len(shapefile))]\n",
    "\n",
    "# tablenames mapping for corresponding region\n",
    "get_tablename = {\n",
    "    'Kenya': 'FEATURES_DB_VIDA_EXTENDED',\n",
    "    'Maharashtra': 'FEATURES_DB_MAHARASHTRA'\n",
    "}\n",
    "\n",
    "# get SQL tablename from which in next steps data will be queried\n",
    "sql_tablename = get_tablename[shapefile.country.iloc[0]]\n",
    "sql_tablename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile = shapefile[shapefile.district.isin(['Lamu', 'Makueni'])]\n",
    "shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_builings_in_bbox(cursor, lon_min, lon_max, lat_min, lat_max):\n",
    "    '''\n",
    "        This particular function is aimed for obtating all entries from defined rectangle for selected SQL table\n",
    "    '''\n",
    "\n",
    "    # fetch column names and their datatypes from defined SQL table\n",
    "    sql = f'''SELECT \n",
    "                COLUMN_NAME, \n",
    "                TYPE_NAME\n",
    "                FROM \"SYSIBM\".\"SQLCOLUMNS\"\n",
    "                WHERE TABLE_SCHEM = 'USER1' AND TABLE_NAME = '{sql_tablename}' '''\n",
    "\n",
    "    cursor.execute(sql)\n",
    "    types_data = cursor.fetchall()\n",
    "\n",
    "    typesmapper = {\n",
    "        'VARCHAR': str, \n",
    "        'DOUBLE': float, \n",
    "        'INTEGER': int, \n",
    "        'SMALLINT':int\n",
    "        }\n",
    "\n",
    "    typesmapper = {str(i[0]).lower() :typesmapper.get(i[1]) for i in types_data}\n",
    "    \n",
    "    columns = list(typesmapper.keys())\n",
    "\n",
    "    # sql statement for selecting entries by defined rectangle boundaries\n",
    "    sql = f\"\"\"\n",
    "        SELECT {', '.join(columns)} FROM USER1.{sql_tablename}\n",
    "        WHERE \n",
    "            (LATITUDE >= {lat_min}) AND \n",
    "            (LATITUDE <= {lat_max}) AND \n",
    "            (LONGITUDE >= {lon_min}) AND \n",
    "            (LONGITUDE <= {lon_max})\n",
    "        \"\"\"\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        data = cursor.fetchall()\n",
    "    except Exception as e:\n",
    "        print(f\"Fetch items error occured: {e}\")\n",
    "        print(\"Reconnevting to the database try again...\")\n",
    "\n",
    "        conn = connect_to_db()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        data = cursor.fetchall()\n",
    "    finally:\n",
    "    #     # reshape obtained data to the GeoDataFrame\n",
    "        df = pd.DataFrame(data=data, columns=columns)\n",
    "        \n",
    "        for col in columns:\n",
    "            df[col] = df[col].astype(typesmapper[col], errors='ignore')\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def create_and_upload_geojson(df, county_metadata):\n",
    "    \n",
    "    # fill None values in certain columns, in order not to get error on frontend side\n",
    "    df['ml_confidence'] = df['ml_confidence'].fillna('-')\n",
    "    df['height'] = df['height'].fillna('-')\n",
    "    df['floors'] = df['floors'].fillna('-')\n",
    "    df['osm_type'] = df['osm_type'].fillna('')\n",
    "    df['gfa_in_meters'] = df['gfa_in_meters'].fillna('-')\n",
    "    df['building_faces'] = df['building_faces'].fillna('-')\n",
    "    df['perimeter_in_meters'] = df['perimeter_in_meters'].fillna('-')\n",
    "    df['elec_access_percent'] = df['elec_access_percent'].fillna('-')\n",
    "    df['elec_consumption_kwh_month'] = df['elec_consumption_kwh_month'].fillna('-')\n",
    "    df['elec_consumption_std_kwh_month'] = df['elec_consumption_std_kwh_month'].fillna('-')\n",
    "\n",
    "    if type(county_metadata.geometry) == shapely.geometry.multipolygon.MultiPolygon:\n",
    "        geometry = shapely.concave_hull(county_metadata.geometry, ratio=1)\n",
    "        county_coordinates = geometry.exterior.coords._coords.tolist()\n",
    "        county_area = abs(geod.geometry_area_perimeter(geometry)[0])\n",
    "    else:\n",
    "        county_coordinates = county_metadata.geometry.exterior.coords._coords.tolist()\n",
    "        county_area = abs(geod.geometry_area_perimeter(county_metadata.geometry)[0])\n",
    "\n",
    "    res_nonres_stats = dict(Counter(df['classification_type']))\n",
    "    rural_urban_stats = dict(Counter(df['urban_split']))\n",
    "\n",
    "    county_properties = {\n",
    "        'count_of_buildings': len(df),\n",
    "        'count_of_buildings_res': res_nonres_stats['res'],\n",
    "        'count_of_buildings_nonRes': res_nonres_stats['non-res'],\n",
    "        'square_area_of_county': county_area,\n",
    "        'square_area_of_buildings': df.area_in_meters.sum(),\n",
    "        'square_area_res': df[df['classification_type'] == 'res'].area_in_meters.sum(),\n",
    "        'square_area_nonRes': df[df['classification_type'] == 'non-res'].area_in_meters.sum(),\n",
    "        'model_confidence_res': df[(df['classification_type'] == 'res') & (df['classification_source'] == 'classification_model')].ml_confidence.mean(),\n",
    "        'model_confidence_nonRes': 1 - df[(df['classification_type'] == 'non-res') & (df['classification_source'] == 'classification_model')].ml_confidence.mean(),\n",
    "        'height_avg': df.height.mean(),\n",
    "        'height_avg_res': df[df['classification_type'] == 'res'].height.mean(),\n",
    "        'height_avg_nonRes': df[df['classification_type'] == 'non-res'].height.mean(),\n",
    "        'county_polygon_coordinates': county_coordinates\n",
    "    }\n",
    "\n",
    "        \n",
    "    if 'Rural' in rural_urban_stats.keys():\n",
    "        county_properties['rural'] = rural_urban_stats['Rural']\n",
    "\n",
    "    if 'Urban' in rural_urban_stats.keys():\n",
    "        county_properties['urban'] = rural_urban_stats['Urban']\n",
    "\n",
    "    if 'Suburban' in rural_urban_stats.keys():\n",
    "        county_properties['suburban'] = rural_urban_stats['Suburban']\n",
    "    \n",
    "\n",
    "    features = []\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            polygon = shapely.from_wkt(row.polygon_coordinates)\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"properties\": {\n",
    "                    \"latitude\": row.latitude,\n",
    "                    \"longitude\": row.longitude,\n",
    "                    \"height\": row.height,\n",
    "                    \"area_in_meters\": row.area_in_meters,\n",
    "                    \"classification_type\": row.classification_type,\n",
    "                    \"type_source\": row.classification_source,\n",
    "                    \"footprint_source\": row.footprint_source,\n",
    "                    \"urban_split\": row.urban_split,\n",
    "                    \"ghsl_smod\": row.ghsl_smod,\n",
    "                    \"floors\": row.floors,\n",
    "                    \"osm_type\": row.osm_type,\n",
    "                    \"gfa_in_meters\": row.gfa_in_meters,\n",
    "                    \"building_faces\": row.building_faces,\n",
    "                    \"perimeter_in_meters\": row.perimeter_in_meters,\n",
    "                    \"elec_access_percent\": row.elec_access_percent,\n",
    "                    \"elec_consumption_kwh_month\": row.elec_consumption_kwh_month,\n",
    "                    \"elec_consumption_std_kwh_month\": row.elec_consumption_std_kwh_month\n",
    "                    },  \n",
    "                \"geometry\": {\n",
    "                    \"coordinates\": [polygon.exterior.coords._coords.tolist()],\n",
    "                    \"type\": \"Polygon\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            if row.classification_source == 'classification_model':\n",
    "                if row.classification_type == 'res':\n",
    "                    feature['properties']['ml_confidence'] = round(row.ml_confidence, 5)\n",
    "                else:\n",
    "                    feature['properties']['ml_confidence'] = round(1 - row.ml_confidence, 5)\n",
    "\n",
    "            if row.footprint_source == 'osm':\n",
    "                feature['properties']['osm_id'] = row.osm_id\n",
    "\n",
    "\n",
    "            features.append(feature)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    try:\n",
    "        geojson = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"county_properties\": county_properties,\n",
    "            \"features\": features\n",
    "        }\n",
    "\n",
    "        filename = f'{county_metadata.country}_{county_metadata.district}.geojson'\n",
    "        file_path = f'{filename}'\n",
    "        with open(file_path, \"w\") as outfile: \n",
    "            json.dump(geojson, outfile)\n",
    "\n",
    "        cos_client.upload_file(Filename=file_path,Bucket=config[\"PRECREATED_GEOJSON_BUCKET\"],Key=filename)\n",
    "        print(f'Json {filename} successfully uploaded to the COS {config[\"PRECREATED_GEOJSON_BUCKET\"]} bucket')\n",
    "        \n",
    "        zip_filename = f'{county_metadata.country}_{county_metadata.district}.zip'\n",
    "        zf = zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED)\n",
    "        zf.write(file_path)\n",
    "        zf.write(\"README.txt\")\n",
    "        zf.close()\n",
    "        cos_client.upload_file(Filename=zip_filename,Bucket=config[\"PRECREATED_GEOJSON_BUCKET\"],Key=zip_filename)\n",
    "        print(f'Zip {zip_filename} successfully uploaded to the COS {config[\"PRECREATED_GEOJSON_BUCKET\"]} bucket')\n",
    "        \n",
    "        os.remove(zip_filename)\n",
    "        os.remove(filename)\n",
    "        \n",
    "        # return geojson\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'functoin create_and_upload_geojson error occurred: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_processed_idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to make geojsons folder\n",
    "try:\n",
    "    os.makedirs('geojsons')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "# try to make geoparquets folder\n",
    "try:\n",
    "    os.makedirs('geoparquets')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_upload_geoparquet(df, county_metadata):\n",
    "    \n",
    "    try:\n",
    "        # save to parquet\n",
    "        filename = f'{county_metadata.country}_{county_metadata.district}.geoparquet'.replace(\"'\", '').replace(\" \", '_')\n",
    "        file_path = f'geoparquets/{filename}'\n",
    "        df.to_parquet(file_path)\n",
    "        \n",
    "        # upload to the IBM COS\n",
    "        cos_client.upload_file(\n",
    "            Filename=file_path,\n",
    "            Bucket=config[\"SHARE_PARQUET_BUCKET\"],\n",
    "            Key=filename,\n",
    "            ExtraArgs={\n",
    "                    'ContentDisposition': 'attachment',\n",
    "                    }\n",
    "        )\n",
    "        \n",
    "        os.remove(file_path)\n",
    "        \n",
    "        print(f'{filename} succesfully uploaded to COS bucket: {config[\"SHARE_PARQUET_BUCKET\"]}')\n",
    "    except Exception as e:\n",
    "        print(f'functoin save_and_upload_geoparquet error occurred: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, county_metadata in enumerate(shapefile.itertuples()):\n",
    "    if idx > last_processed_idx:\n",
    "        print()\n",
    "        print(f'Processing county: {county_metadata.district} {idx+1} of 47')\n",
    "        # (minx, miny, maxx, maxy)\n",
    "        min_lon, min_lat, max_lon, max_lat = county_metadata.geometry.bounds\n",
    "\n",
    "        df = fetch_builings_in_bbox(cursor, min_lon, max_lon, min_lat, max_lat)\n",
    "        \n",
    "        df['buildings_in_polygon'] = [county_metadata.geometry.contains(shapely.Point(row.longitude, row.latitude)) for row in tqdm(df.itertuples(), total=len(df), desc='Filtering buildings')]\n",
    "\n",
    "        df = df[df.buildings_in_polygon == True]\n",
    "        df = df.drop(['buildings_in_polygon'], axis=1)\n",
    "\n",
    "        \n",
    "        print(f'buildings in polygoon {len(df)}')\n",
    "        save_and_upload_geoparquet(df, county_metadata)\n",
    "        \n",
    "        create_and_upload_geojson(df, county_metadata)\n",
    "        \n",
    "        \n",
    "        last_processed_idx = idx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdistricts_mapping = {}\n",
    "\n",
    "for idx, county_metadata in enumerate(shapefile.itertuples()):\n",
    "\n",
    "    # print(idx)\n",
    "    district = county_metadata.county.replace(\"'\", '').replace(\" \", '_')\n",
    "\n",
    "    if county_metadata.country not in subdistricts_mapping.keys():\n",
    "        subdistricts_mapping[county_metadata.country] = {}\n",
    "    # subdistrict = county_metadata.sdtname.replace('(', '').replace(')', '').replace(' ', '_')\n",
    "    # subdistrict_sufix = '' if county_metadata.Index[1] == 0 else f'_{county_metadata.Index[1] + 1}'\n",
    "\n",
    "    filename = f'{county_metadata.country}_{district}.json'\n",
    "    subdistricts_mapping[county_metadata.country][district] = filename\n",
    "    # print({f'{county_metadata.country}': {f'{district}' : filename}})\n",
    "        \n",
    "subdistricts_mapping\n",
    "\n",
    "file_path = f'geojson_subdistricts_map.json'\n",
    "with open(file_path, \"w\") as outfile: \n",
    "    json.dump(subdistricts_mapping, outfile)\n",
    "\n",
    "subdistricts_mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
