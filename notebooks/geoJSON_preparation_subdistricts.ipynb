{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geoJSON_preparation_subdistrict\n",
    "### This notebook generates the geoJSON files related to subdistricts used in the OBI tool in the \"Select an area\" option\n",
    "### last_processed_idx can be used to resume a failed computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial configuration\n",
    "#### To start working with this particular notebook, you need to provide necessary credential and settings\n",
    "#### Below is an template of configuration, which is necessary prepare aside of this notebook and copy & paste all content in triple quotes to the next cell's input field\n",
    "    \"\"\"\n",
    "    {\n",
    "    \"COS_ENDPOINT_URL\": \"s3.private.eu-de.cloud-object-storage.appdomain.cloud\",\n",
    "    \"COS_AUTH_ENDPOINT_URL\": \"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    \"COS_APIKEY\": \"xxx\",\n",
    "    \"PRECREATED_GEOJSON_BUCKET\": \"counties-geojsons\",\n",
    "    \"DB2_CONNECTION_STRING\": \"jdbc:db2://65beb513-5d3d-4101-9001-f42e9dc954b3.brt9d04f0cmqeb8u7740.databases.appdomain.cloud:30371/BLUDB:sslConnection=true;useJDBC4ColumnNameAndLabelSemantics=false;db2.jcc.charsetDecoderEncoder=3;\",\n",
    "    \"DB2_USERNAME\": \"xxx\",\n",
    "    \"DB2_PASSWORD\": \"xxx\",\n",
    "    \"COUNTRY_TABLE\": \"FEATURES_DB_VIDA_EXTENDED\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read notebook configuration\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "config_str = getpass.getpass('Enter your prepared config: ')\n",
    "config = json.loads(config_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import geopandas as gpd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapely\n",
    "from collections import Counter\n",
    "import jaydebeapi as jdbc\n",
    "import jpype\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from botocore.client import Config\n",
    "from pyproj import Geod\n",
    "import ibm_boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init S3 client in order to work with last tiff file version\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "                              ibm_api_key_id=config[\"COS_APIKEY\"],\n",
    "                              ibm_auth_endpoint=config[\"COS_AUTH_ENDPOINT_URL\"],\n",
    "                              config=Config(signature_version='oauth'),\n",
    "                              endpoint_url=config[\"COS_ENDPOINT_URL\"])\n",
    "geod = Geod(ellps=\"WGS84\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startJVM exception:  JVM is already started\n"
     ]
    }
   ],
   "source": [
    "def connect_to_db():\n",
    "\n",
    "    jar = 'db2jcc4.jar'\n",
    "    os.environ['CLASSPATH'] = jar\n",
    "\n",
    "    args='-Djava.class.path=%s' % jar\n",
    "    jvm_path = jpype.getDefaultJVMPath()\n",
    "    try:\n",
    "        jpype.startJVM(jvm_path, args)\n",
    "    except Exception as e:\n",
    "        print('startJVM exception: ', e)\n",
    "        \n",
    "    if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n",
    "        jpype.attachThreadToJVM()\n",
    "        jpype.java.lang.Thread.currentThread().setContextClassLoader(jpype.java.lang.ClassLoader.getSystemClassLoader())\n",
    "        \n",
    "    # create JDBC connection\n",
    "    conn = jdbc.connect(\n",
    "                'com.ibm.db2.jcc.DB2Driver',\n",
    "                config['DB2_CONNECTION_STRING'],\n",
    "                [config[\"DB2_USERNAME\"], config[\"DB2_PASSWORD\"]],\n",
    "                'db2jcc4.jar')\n",
    "    \n",
    "    return conn\n",
    "\n",
    "conn = connect_to_db()\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile = gpd.read_file(open('boundaries/MAHARASHTRA_SUBDISTRICTS.geojson'))\n",
    "shapefile['country'] = ['Maharashtra' for _ in  range(len(shapefile))]\n",
    "shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = [\n",
    "    'Ahmadnagar', \n",
    "    'Aurangabad', \n",
    "    'Kolhapur', \n",
    "    'Nagpur', \n",
    "    'Nashik', \n",
    "    'Pune', \n",
    "    'Satara', \n",
    "    'Thane',\n",
    "    'Solapur'\n",
    "]\n",
    "\n",
    "selected_districts = shapefile[shapefile.dtname.isin(districts)].explode(index_parts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_builings_in_bbox(lon_min, lon_max, lat_min, lat_max):\n",
    "\n",
    "    columns = [\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'polygon_coordinates',\n",
    "        'height',\n",
    "        'ml_confidence',\n",
    "        'area_in_meters',\n",
    "        'urban_split',\n",
    "        'classification_source',\n",
    "        'classification_type',\n",
    "        'footprint_source',\n",
    "        'osm_id',\n",
    "        'ghsl_smod',\n",
    "        'floors',\n",
    "        'osm_type',\n",
    "        'gfa_in_meters',\n",
    "        'building_faces',\n",
    "        'perimeter_in_meters',\n",
    "        'elec_access_percent',\n",
    "        'elec_consumption_kwh_month',\n",
    "        'elec_consumption_std_kwh_month'\n",
    "    ]\n",
    "    sql = f\"\"\"\n",
    "        SELECT {', '.join(columns)} FROM USER1.FEATURES_DB_MAHARASHTRA\n",
    "        WHERE \n",
    "            (LATITUDE >= {lat_min}) AND \n",
    "            (LATITUDE <= {lat_max}) AND \n",
    "            (LONGITUDE >= {lon_min}) AND \n",
    "            (LONGITUDE <= {lon_max})\n",
    "            \"\"\"\n",
    "    # cursor = conn.cursor()\n",
    "    cursor.execute(sql)\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    gpd.options.display_precision = 7 \n",
    "\n",
    "    df = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "    convert_dict = {\n",
    "                    'latitude': float,\n",
    "                    'longitude': float,\n",
    "                    'polygon_coordinates': str,\n",
    "                    'height': float,\n",
    "                    'ml_confidence': float,\n",
    "                    'area_in_meters': float,\n",
    "                    'urban_split': str,\n",
    "                    'classification_source': str,\n",
    "                    'classification_type': str,\n",
    "                    'footprint_source': str,\n",
    "                    'osm_id': int,\n",
    "                    'ghsl_smod': str,\n",
    "                    'osm_type': str,\n",
    "                    'floors': int,\n",
    "                    'gfa_in_meters': float,\n",
    "                    'building_faces': int,\n",
    "                    'perimeter_in_meters': float,\n",
    "                    'elec_access_percent': float,\n",
    "                    'elec_consumption_kwh_month': float,\n",
    "                    'elec_consumption_std_kwh_month': float\n",
    "                    }\n",
    "\n",
    "    df = df.astype(convert_dict)\n",
    "    # df['geometry'] = gpd.GeoSeries.from_wkt(df['polygon_coordinates'])\n",
    "    # df = df.drop(columns=['polygon_coordinates'])\n",
    "\n",
    "    # df = gpd.GeoDataFrame(\n",
    "    #     df, geometry=df.geometry, crs=\"EPSG:4326\"\n",
    "    # )\n",
    "\n",
    "    # df = df.where(~df['geometry'].isna()).dropna()\n",
    "\n",
    "    df \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_geojson(df, county_metadata):\n",
    "\n",
    "    if type(county_metadata.geometry) == shapely.geometry.multipolygon.MultiPolygon:\n",
    "        geometry = shapely.concave_hull(county_metadata.geometry, ratio=1)\n",
    "        county_coordinates = geometry.exterior.coords._coords.tolist()\n",
    "        county_area = abs(geod.geometry_area_perimeter(geometry)[0])\n",
    "    else:\n",
    "        county_coordinates = county_metadata.geometry.exterior.coords._coords.tolist()\n",
    "        county_area = abs(geod.geometry_area_perimeter(county_metadata.geometry)[0])\n",
    "\n",
    "    res_nonres_stats = dict(Counter(df['classification_type']))\n",
    "    rural_urban_stats = dict(Counter(df['urban_split']))\n",
    "\n",
    "    county_properties = {\n",
    "        'count_of_buildings': len(df),\n",
    "        'count_of_buildings_res': res_nonres_stats['res'],\n",
    "        'count_of_buildings_nonRes': res_nonres_stats['non-res'],\n",
    "        'square_area_of_county': county_area,\n",
    "        'square_area_of_buildings': df.area_in_meters.sum(),\n",
    "        'square_area_res': df[df['classification_type'] == 'res'].area_in_meters.sum(),\n",
    "        'square_area_nonRes': df[df['classification_type'] == 'non-res'].area_in_meters.sum(),\n",
    "        'model_confidence_res': df[(df['classification_type'] == 'res') & (df['classification_source'] == 'classification_model')].ml_confidence.mean(),\n",
    "        'model_confidence_nonRes': 1 - df[(df['classification_type'] == 'non-res') & (df['classification_source'] == 'classification_model')].ml_confidence.mean(),\n",
    "        'height_avg': df.height.mean(),\n",
    "        'height_avg_res': df[df['classification_type'] == 'res'].height.mean(),\n",
    "        'height_avg_nonRes': df[df['classification_type'] == 'non-res'].height.mean(),\n",
    "        'county_polygon_coordinates': county_coordinates\n",
    "    }\n",
    "\n",
    "        \n",
    "    if 'Rural' in rural_urban_stats.keys():\n",
    "        county_properties['rural'] = rural_urban_stats['Rural']\n",
    "\n",
    "    if 'Urban' in rural_urban_stats.keys():\n",
    "        county_properties['urban'] = rural_urban_stats['Urban']\n",
    "\n",
    "    if 'Suburban' in rural_urban_stats.keys():\n",
    "        county_properties['suburban'] = rural_urban_stats['Suburban']\n",
    "    \n",
    "\n",
    "    features = []\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            polygon = shapely.from_wkt(row.polygon_coordinates)\n",
    "            feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"properties\": {\n",
    "                    \"latitude\": row.latitude,\n",
    "                    \"longitude\": row.longitude,\n",
    "                    \"height\": row.height,\n",
    "                    \"area_in_meters\": row.area_in_meters,\n",
    "                    \"classification_type\": row.classification_type,\n",
    "                    \"classification_source\": row.classification_source,\n",
    "                    \"footprint_source\": row.footprint_source,\n",
    "                    \"urban_split\": row.urban_split,\n",
    "                    \"ghsl_smod\": row.ghsl_smod,\n",
    "                    \"floors\": row.floors,\n",
    "                    \"osm_type\": row.osm_type,\n",
    "                    \"gfa_in_meters\": row.gfa_in_meters,\n",
    "                    \"building_faces\": row.building_faces,\n",
    "                    \"perimeter_in_meters\": row.perimeter_in_meters,\n",
    "                    \"elec_access_percent\": row.elec_access_percent,\n",
    "                    \"elec_consumption_kwh_month\": row.elec_consumption_kwh_month,\n",
    "                    \"elec_consumption_std_kwh_month\": row.elec_consumption_std_kwh_month\n",
    "                    },  \n",
    "                \"geometry\": {\n",
    "                    \"coordinates\": [polygon.exterior.coords._coords.tolist()],\n",
    "                    \"type\": \"Polygon\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            if row.classification_source == 'classification_model':\n",
    "                if row.classification_type == 'res':\n",
    "                    feature['properties']['ml_confidence'] = round(row.ml_confidence, 5)\n",
    "                else:\n",
    "                    feature['properties']['ml_confidence'] = round(1 - row.ml_confidence, 5)\n",
    "\n",
    "            if row.footprint_source == 'osm':\n",
    "                feature['properties']['osm_id'] = row.osm_id\n",
    "\n",
    "\n",
    "            features.append(feature)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"county_properties\": county_properties,\n",
    "    \"features\": features\n",
    "    }\n",
    "\n",
    "    filename = f'{county_metadata.country}_{county_metadata.district}.json'\n",
    "    file_path = f'geojsons/{filename}'\n",
    "    with open(file_path, \"w\") as outfile: \n",
    "        json.dump(geojson, outfile)\n",
    "\n",
    "    cos_client.upload_file(\n",
    "        Filename=file_path,\n",
    "        Bucket=config[\"PRECREATED_GEOJSON_BUCKET\"],\n",
    "        Key=filename.replace(\"'\", '').replace(\" \", '_'),\n",
    "        ExtraArgs={\n",
    "                'ContentDisposition': 'attachment',\n",
    "                }\n",
    "        )\n",
    "    print(f'File {filename} successfully uploaded to the COS {config[\"PRECREATED_GEOJSON_BUCKET\"]} bucket')\n",
    "    # return geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main for cycle for interating through district polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_processed_idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startJVM exception:  JVM is already started\n"
     ]
    }
   ],
   "source": [
    "conn = connect_to_db()\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, county_metadata in enumerate(selected_districts.itertuples()):\n",
    "    if idx > last_processed_idx:\n",
    "        print()\n",
    "        print(f'Processing subdistrict: {county_metadata.sdtname} {idx+1} of {len(selected_districts)}')\n",
    "        # (minx, miny, maxx, maxy)\n",
    "        min_lon, min_lat, max_lon, max_lat = county_metadata.geometry.bounds\n",
    "\n",
    "        df = fetch_builings_in_bbox(min_lon, max_lon, min_lat, max_lat)\n",
    "\n",
    "        buildings_in_polygon = []\n",
    "        df['buildings_in_polygon'] = [county_metadata.geometry.contains(shapely.Point(row.longitude, row.latitude)) for row in tqdm(df.itertuples(), total=len(df), desc='Filtering buildings')]\n",
    "\n",
    "        df = df[df.buildings_in_polygon == True]\n",
    "        \n",
    "        # fill None values in certain columns, in order not to get error on frontend side\n",
    "        df['ml_confidence'] = df['ml_confidence'].fillna('-')\n",
    "        df['height'] = df['height'].fillna('-')\n",
    "        df['floors'] = df['floors'].fillna('-')\n",
    "        df['osm_type'] = df['osm_type'].fillna('')\n",
    "        df['gfa_in_meters'] = df['gfa_in_meters'].fillna('-')\n",
    "        df['building_faces'] = df['building_faces'].fillna('-')\n",
    "        df['perimeter_in_meters'] = df['perimeter_in_meters'].fillna('-')\n",
    "        df['elec_access_percent'] = df['elec_access_percent'].fillna('-')\n",
    "        df['elec_consumption_kwh_month'] = df['elec_consumption_kwh_month'].fillna('-')\n",
    "        df['elec_consumption_std_kwh_month'] = df['elec_consumption_std_kwh_month'].fillna('-')\n",
    "\n",
    "        print(f'buildings in polygoon {len(df)}')\n",
    "        create_geojson(df, county_metadata)\n",
    "\n",
    "        last_processed_idx = idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f'geojson_subdistricts_map.json'\n",
    "\n",
    "subdistricts_mapping = json.loads(open(file_path, \"rb\").read())\n",
    "\n",
    "for idx, county_metadata in enumerate(selected_districts.itertuples()):\n",
    "\n",
    "    if county_metadata.country not in subdistricts_mapping.keys():\n",
    "        subdistricts_mapping[county_metadata.country] = {}\n",
    "\n",
    "    district = county_metadata.dtname.replace(\"'\", '').replace(\" \", '_')\n",
    "    subdistrict = county_metadata.sdtname.replace('(', '').replace(')', '').replace(' ', '_')\n",
    "    subdistrict_sufix = '' if county_metadata.Index[1] == 0 else f'_{county_metadata.Index[1] + 1}'\n",
    "\n",
    "    filename = f'{county_metadata.country}_{district}_{subdistrict}{subdistrict_sufix}.json'\n",
    "\n",
    "    if type(subdistricts_mapping[county_metadata.country][district]) == str:\n",
    "        subdistricts_mapping[county_metadata.country][district] = {f'{subdistrict}{subdistrict_sufix}': filename}\n",
    "\n",
    "    else:\n",
    "        subdistricts_mapping[county_metadata.country][district][f'{subdistrict}{subdistrict_sufix}'] = filename\n",
    "        \n",
    "\n",
    "subdistricts_mapping\n",
    "file_path = f'geojson_subdistricts_map.json'\n",
    "with open(file_path, \"w\") as outfile: \n",
    "    json.dump(subdistricts_mapping, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maharashtra_Garhchiroli.json\n",
      "Maharashtra_Gondiya.json\n",
      "Maharashtra_Latur.json\n",
      "Maharashtra_Pune.json\n",
      "Maharashtra_Sindhudurg.json\n",
      "Maharashtra_Thane.json\n",
      "Maharashtra_Wardha.json\n",
      "Maharashtra_Washim.json\n",
      "Maharashtra_Yavatmal.json\n",
      "Maharashtra_Kolhapur.json\n",
      "Maharashtra_Nagpur.json\n",
      "Maharashtra_Parbhani.json\n",
      "Maharashtra_Ahmadnagar.json\n",
      "Maharashtra_Akola.json\n",
      "Maharashtra_Aurangabad.json\n",
      "Maharashtra_Bid.json\n",
      "Maharashtra_Buldana.json\n",
      "Maharashtra_Chandrapur.json\n",
      "Maharashtra_Dhule.json\n",
      "Maharashtra_Nanded.json\n",
      "Maharashtra_Nandurbar.json\n",
      "Maharashtra_Hingoli.json\n",
      "Maharashtra_Nashik.json\n",
      "Maharashtra_Osmanabad.json\n",
      "Maharashtra_Raigarh.json\n",
      "Maharashtra_Ratnagiri.json\n",
      "Maharashtra_Solapur.json\n",
      "Maharashtra_Sangli.json\n",
      "Maharashtra_Satara.json\n",
      "Maharashtra_Amravati.json\n",
      "Maharashtra_Bhandara.json\n",
      "Maharashtra_Mumbai_Suburban.json\n",
      "Maharashtra_Jalgaon.json\n",
      "Maharashtra_Jalna.json\n",
      "Maharashtra_Mumbai.json\n"
     ]
    }
   ],
   "source": [
    "for idx, county_metadata in enumerate(shapefile.itertuples()):\n",
    "    print('Maharashtra_'+county_metadata.district.replace(\"'\", '').replace(\" \", '_')+'.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_client.upload_file(\n",
    "        Filename='geojsons_subdistricts/Maharashtra_Solapur_Mangalvedhe.json',\n",
    "        Bucket=config[\"PRECREATED_GEOJSON_BUCKET\"],\n",
    "        Key='Maharashtra_Solapur_Mangalvedhe.json',\n",
    "        ExtraArgs={\n",
    "                'ContentDisposition': 'attachment',\n",
    "                }\n",
    "        )\n",
    "# print(f'File {file_path} successfully uploaded to the COS {config[\"PRECREATED_GEOJSON_BUCKET\"]} bucket')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
